{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 4 - Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmjcRV2S3vTy",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 4: Preprocessing\n",
        "\n",
        "## 0. Loading the data\n",
        "\n",
        "First we mount our GDrive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaGeyH4rz3wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount GDrive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LRuOfyHafHT",
        "colab_type": "text"
      },
      "source": [
        "After mounting our GDrive, we load the data on a Pandas DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AStXx7Mh3y8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data into a Pandas DataFrame\n",
        "review_df = pd.read_csv(\"/content/drive/My Drive/TFM/yelp_reviews.csv\")\n",
        "\n",
        "# Show 5 columns as an example\n",
        "review_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_UEcg6h3x4i",
        "colab_type": "text"
      },
      "source": [
        "## 1. Case normalization\n",
        "\n",
        "For each review, we apply Pandas method to lower-case:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC5RW8In30D7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lower-case all text column via Pandas method\n",
        "review_df['text'] = review_df['text'].str.lower()\n",
        "\n",
        "# Show 5 rows as an example\n",
        "review_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97TO3Lzw36H1",
        "colab_type": "text"
      },
      "source": [
        "## 2. Tokenization\n",
        "\n",
        "Tokenization is conducted via WordPunctTokenizer of NLTK:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIycDkTD32H0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the tokenizer from NLTK\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "# Initiate the tokenizer class\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "# Apply the tokenizer to each row\n",
        "review_df['text'] = review_df['text'].apply(lambda x: tokenizer.tokenize(x))\n",
        "\n",
        "# Show 5 rows as an example\n",
        "review_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iB3b5PQ393v",
        "colab_type": "text"
      },
      "source": [
        "## 3. Stopping\n",
        "\n",
        "### 3.1. Removal of stop words\n",
        "\n",
        "If it is your first time loading NLTK's stop words list, you should run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kit970ri3_kP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You will have to download the set of stop words the first time\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvw-DF--cpPY",
        "colab_type": "text"
      },
      "source": [
        "Now we just have to remove words from reviews that are in the stop words list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rQaRaJ94B1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load stopwords from NLTK\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load list of stopwords for English\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Remove stop words for each row\n",
        "review_df['text'] = review_df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# Show 5 rows as an example\n",
        "review_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89EDGDqC4FLa",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Removal of non-characters\n",
        "\n",
        "We use Python's built-in method to remove non-characters:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W1Jitl04DKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove non-characters for each row\n",
        "review_df['text'] = review_df['text'].apply(lambda x: [word for word in x if word.isalpha()])\n",
        "\n",
        "# Show 5 rows as an example\n",
        "review_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwQCo8c_4ItV",
        "colab_type": "text"
      },
      "source": [
        "## 4. Spelling normalization\n",
        "\n",
        "We are going to use pyspellchecker library to run spelling normalization. If you do not have this library on your system, please run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl0FlyI34KBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell if you do not have the pyspellchecker library\n",
        "pip install pyspellchecker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozn1NOTcd6rj",
        "colab_type": "text"
      },
      "source": [
        "Now we can use the SpellChecker method to correct spelling.\n",
        "\n",
        "__Warning:__ this process takes a long time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-ZDw95g4Lue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import method from pyspellchecker library\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Initiate SpellChecker class\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Correct spelling for each row\n",
        "review_df['text'] = review_df['text'].apply(lambda x: [spell.correction(word) for word in x])\n",
        "\n",
        "# Show 5 rows as an example\n",
        "review_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAf9A2UV4Ot7",
        "colab_type": "text"
      },
      "source": [
        "## 5. Stemming\n",
        "\n",
        "We use the EnglishStemmer from the snowball module of NLTK's stemmers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz2Xs_pO4NLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the stemmer from NLTK\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "\n",
        "# Initiate stemmer class\n",
        "sb = EnglishStemmer()\n",
        "\n",
        "# Stem for each row\n",
        "review_df['text'] = review_df['text'].apply(lambda x: [sb.stem(word) for word in x])\n",
        "\n",
        "# Show 5 rows as an example\n",
        "review_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOCeJuZR4Trd",
        "colab_type": "text"
      },
      "source": [
        "## 6. Word embedding\n",
        "\n",
        "### 6.1. GloVe\n",
        "\n",
        "First, we must import our pretrained GloVe model, which is just a dictionary of words with its word embedding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbmX4V2-4T6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "\n",
        "# Initialize dictionary of GloVe word embeddings\n",
        "glove_embeddings = {}\n",
        "# Open file of trained GloVe model and store it on this dictionary\n",
        "with open(\"/content/drive/My Drive/TFM/glove.twitter.27B.100d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        glove_embeddings[word] = vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt8oJaMrpU2-",
        "colab_type": "text"
      },
      "source": [
        "We discard words that are not in our GloVe pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTJtZsiV4WqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discard words that are not in our GloVe pretrained models\n",
        "review_df['text'] = review_df['text'].apply(lambda x: [word for word in x if word in glove_embeddings.keys()])\n",
        "\n",
        "# Show 5 rows as an example\n",
        "review_df['text'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnbSQwSd4a2l",
        "colab_type": "text"
      },
      "source": [
        "### 6.2. TF-IDF\n",
        "\n",
        "We apply sklearn's TF-IDF method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvRqL1uq4cVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load method from sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# sklearn's method takes as input a list of documents. We create a list of reviews\n",
        "corpus = [\" \".join(review) for review in review_df['text']]\n",
        "\n",
        "# Initiate TF-IDF class\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Compute TF-IDF\n",
        "tf_idf_reviews = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3vEXoan4n9p",
        "colab_type": "text"
      },
      "source": [
        "### 6.3. GloVe averaged by TF-IDF\n",
        "\n",
        "We compute GloVe averaged by TF-IDF and store it to our original review DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCbjUHAV4ett",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get words in the TF-IDF model\n",
        "tf_idf_word_column = vectorizer.get_feature_names()\n",
        "\n",
        "# Initialize list for final output\n",
        "glove_tf_idf_reviews = []\n",
        "\n",
        "# Compute GloVe averaged by TF-IDF\n",
        "for row, review in enumerate(review_df['text']):\n",
        "  \n",
        "  # Temporal list to store results\n",
        "  tmp = []\n",
        "\n",
        "  # For each word in a review, if the word is in the TF-IDF model, create a list of word*TF-IDF term \n",
        "  for word in review:\n",
        "    if word in tf_idf_word_column:\n",
        "      tmp.append(glove_embeddings[word]*tf_idf_reviews[row,tf_idf_word_column.index(word)])\n",
        "\n",
        "  # Sum the weighted\n",
        "  glove_tf_idf_reviews.append(sum(tmp))\n",
        "\n",
        "# Save output to original DataFrame\n",
        "review_df['text'] = glove_tf_idf_reviews\n",
        "\n",
        "# Show 5 rows as an example\n",
        "review_df['text'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvJXRyEn41MU",
        "colab_type": "text"
      },
      "source": [
        "## 7. Export\n",
        "\n",
        "Finally, we export the preprocessed dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSLHOxQG4nzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Export the DataFrame to a csv\n",
        "review_df.to_csv(\"/content/drive/My Drive/TFM/yelp_final_data.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}